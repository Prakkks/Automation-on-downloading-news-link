{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd\n",
    "\n",
    "def is_social_media_url(url):\n",
    "    social_media_domains = ['twitter.com', 'facebook.com', 'instagram.com']  # Add more if needed\n",
    "    for domain in social_media_domains:\n",
    "        if domain in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_all_links(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract all links using BeautifulSoup methods\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "            # Convert relative URLs to absolute URLs\n",
    "            links = [urljoin(url, link) for link in links]\n",
    "\n",
    "            return links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from '{url}'. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_links_to_csv(links, csv_filename):\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Link'])  # Write header\n",
    "\n",
    "        for link in links:\n",
    "            csv_writer.writerow([link])\n",
    "\n",
    "def extract_specific_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Skip extraction for social media URLs\n",
    "        if is_social_media_url(url):\n",
    "            print(f\"Skipping {url} because it's a social media link.\")\n",
    "            return None, None, None, None, None, None  # Include the original link as None\n",
    "\n",
    "        heading_element = soup.find('h1', {'style': 'margin-bottom:0.1rem;'})\n",
    "        author_element = soup.find('h5', class_='text-capitalize')\n",
    "        publication_date_element = soup.find('div', class_='updated-time')\n",
    "        content_container = soup.find('div', class_='subscribe--wrapperx')\n",
    "\n",
    "        # Determine category based on URL\n",
    "        url_parts = urlparse(url).path.split('/')\n",
    "        category = next((part for part in url_parts if part), 'Category not found')\n",
    "\n",
    "        heading = heading_element.text.strip() if heading_element else 'Heading not found'\n",
    "        author = author_element.text.strip() if author_element else 'Author not found'\n",
    "        publication_date_raw = publication_date_element.text.strip() if publication_date_element else 'Date not found'\n",
    "        publication_date = publication_date_raw.replace('Published at :', '').strip()\n",
    "        content = content_container.get_text(separator=' ', strip=True) if content_container else 'Content not found'\n",
    "\n",
    "        return heading, author, publication_date, content, url, category\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve content from {url}. Error: {e}\")\n",
    "        return None, None, None, None, url, None  # Include the original link and None for category\n",
    "\n",
    "\n",
    "def save_to_csv(data, csv_file_path):\n",
    "    with open(csv_file_path, 'a', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Heading', 'Author', 'Publication_date', 'Source', 'Content', 'Link', 'Category'])  # Updated header\n",
    "        csv_writer.writerow(data)\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'kathmandupost.csv'\n",
    "\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Heading', 'Author', 'Publication_date', 'Source', 'Content', 'Link', 'Category'])  # Updated header\n",
    "\n",
    "    url = 'https://kathmandupost.com'\n",
    "    links = extract_all_links(url)\n",
    "\n",
    "    # Save links to CSV file\n",
    "    csv_filename = 'data_links.csv'\n",
    "    save_links_to_csv(links, csv_filename)\n",
    "\n",
    "    # Print a message indicating success\n",
    "    print(f\"All links saved to {csv_filename}\")\n",
    "\n",
    "    link = pd.read_csv('data_links.csv')\n",
    "\n",
    "    for url in links:\n",
    "        if not urlparse(url).scheme:\n",
    "            url = urljoin('https://', url)\n",
    "\n",
    "        if urlparse(url).netloc and not is_social_media_url(url):\n",
    "            heading, author, publication_date, content, link, category = extract_specific_content(url)\n",
    "            if heading is not None and author is not None and content is not None:\n",
    "                data_to_save = [heading, author, publication_date, 'Kathmandu-Post', content, link, category]\n",
    "                print(f\"Data for {url}:\\nHeading: {heading}\\nAuthor: {author}\\nPublication Date: {publication_date}\\nSource: Kathmandu-Post\\nContent: {content}\\nLink: {link}\\nCategory: {category}\\n\")\n",
    "                save_to_csv(data_to_save, csv_file_path)\n",
    "                print(f\"Data saved for {url}\")\n",
    "        else:\n",
    "            print(f\"Invalid URL format or social media link: {url}\")\n",
    "\n",
    "if __name__ == \"_main_\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
